{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Prepare Environment"
      ],
      "metadata": {
        "id": "oHHWMKfixWM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Restart after this step. Then, continue with Step 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "P4gjAIFrxvno"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO-k-WB0u_p-"
      },
      "outputs": [],
      "source": [
        "%pip install  --upgrade \\\n",
        "  \"torch>=2.4.0\" \\\n",
        "  tensorboard \\\n",
        "  \"transformers>=4.51.3\" \\\n",
        "  datasets \\\n",
        "  \"accelerate==1.4.0\" \\\n",
        "  \"evaluate==0.4.3\" \\\n",
        "  \"bitsandbytes==0.45.3\" \\\n",
        "  \"trl==0.21.0\" \\\n",
        "  \"peft==0.14.0\" \\\n",
        "  \"protobuf==3.20.3\" \\\n",
        "  sentencepiece \\\n",
        "  evaluate \\\n",
        "  sacrebleu \\\n",
        "  rouge-score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Enter your Hugging Face access token to be able to use Gemma"
      ],
      "metadata": {
        "id": "3jzREZjQyYpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ],
      "metadata": {
        "id": "SzDdln2mylw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Create a combined dataset"
      ],
      "metadata": {
        "id": "PC5OkkhJx-R3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "import random\n",
        "\n",
        "# Fix random seed for reproducibility\n",
        "random.seed(22)"
      ],
      "metadata": {
        "id": "YlYsAgrozUmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2.1: Load datasets"
      ],
      "metadata": {
        "id": "2UnuCPdDyORk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "alpaca = load_dataset(\"tatsu-lab/alpaca\")\n",
        "tulu = load_dataset(\"allenai/tulu-v2-sft-mixture\")\n",
        "ultra = load_dataset(\"HuggingFaceH4/ultrachat_200k\")"
      ],
      "metadata": {
        "id": "DerUZe84x0YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2.2: Sample and split datasets"
      ],
      "metadata": {
        "id": "fufS3T3Mz1Sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Samples and splits datasets\n",
        "def sample_split(dataset, train_size=5000, test_size=2000):\n",
        "  indexes = list(range(len(dataset)))\n",
        "  random.shuffle(indexes)\n",
        "\n",
        "  train_indexes = indexes[:train_size]\n",
        "  test_indexes = indexes[train_size:train_size + test_size]\n",
        "\n",
        "  train_split = dataset.select(train_indexes)\n",
        "  test_split = dataset.select(test_indexes)\n",
        "\n",
        "  return train_split, test_split\n",
        "\n",
        "\n",
        "alpaca_train, alpaca_test = sample_split(alpaca[\"train\"])\n",
        "tulu_train, tulu_test = sample_split(tulu[\"train\"])\n",
        "ultrachat_train, ultrachat_test = sample_split(ultra[\"train_sft\"])"
      ],
      "metadata": {
        "id": "uqzT74uezbvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2.3: Format datasets as system, instruction, input, and response"
      ],
      "metadata": {
        "id": "sPFfRuvt0kDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format alpaca sample as system, instruction, input, and response\n",
        "def format_alpaca(row):\n",
        "  system = \"\"\n",
        "  instruction = row[\"instruction\"]\n",
        "  input = row[\"input\"].strip()\n",
        "  response = row[\"output\"]\n",
        "\n",
        "  chat = [\n",
        "      {\"system\": system, \"instruction\": instruction, \"input\": input, \"response\": response}\n",
        "  ]\n",
        "\n",
        "  return {\"chat\": chat}\n",
        "\n",
        "\n",
        "# Format tulu sample as system, instruction, input, and response\n",
        "def format_tulu(row):\n",
        "  chat = []\n",
        "  i = 0\n",
        "  system = \"\"\n",
        "  input = \"\"\n",
        "  while i < len(row[\"messages\"]):\n",
        "    role = row[\"messages\"][i][\"role\"]\n",
        "    if role == \"system\":\n",
        "      system = row[\"messages\"][i][\"content\"]\n",
        "    elif role == \"user\":\n",
        "      instruction = row[\"messages\"][i][\"content\"]\n",
        "    elif role == \"assistant\":\n",
        "      response = row[\"messages\"][i][\"content\"]\n",
        "    i += 1\n",
        "\n",
        "  chat.append({\"system\": system, \"instruction\": instruction, \"input\": input, \"response\": response})\n",
        "\n",
        "  return {\"chat\": chat}\n",
        "\n",
        "\n",
        "# Format ultrachat sample as system, instruction, input, and response\n",
        "def format_ultrachat(row):\n",
        "  chat = []\n",
        "  i = 0\n",
        "  system = \"\"\n",
        "  input = \"\"\n",
        "  while i < len(row[\"messages\"]):\n",
        "    role = row[\"messages\"][i][\"role\"]\n",
        "    if role == \"user\":\n",
        "      instruction = row[\"messages\"][i][\"content\"]\n",
        "    elif role == \"assistant\":\n",
        "      response = row[\"messages\"][i][\"content\"]\n",
        "    else: print(\"****\")\n",
        "    i += 1\n",
        "\n",
        "  chat.append({\"system\": system, \"instruction\": instruction, \"input\": input, \"response\": response})\n",
        "\n",
        "  return {\"chat\": chat}\n",
        "\n",
        "\n",
        "# Format and map training samples into dataset format\n",
        "alpaca_train_mapped = alpaca_train.map(format_alpaca, remove_columns=alpaca_train.column_names)\n",
        "tulu_train_mapped= tulu_train.map(format_tulu, remove_columns=tulu_train.column_names)\n",
        "ultrachat_train_mapped= ultrachat_train.map(format_ultrachat, remove_columns=ultrachat_train.column_names)\n",
        "\n",
        "# Format and map test samples into dataset format\n",
        "alpaca_test_mapped = alpaca_test.map(format_alpaca, remove_columns=alpaca_test.column_names)\n",
        "tulu_test_mapped= tulu_test.map(format_tulu, remove_columns=tulu_test.column_names)\n",
        "ultrachat_test_mapped= ultrachat_test.map(format_ultrachat, remove_columns=ultrachat_test.column_names)"
      ],
      "metadata": {
        "id": "sQMcFNSz0jW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2.4: Combine datasets"
      ],
      "metadata": {
        "id": "wsnOFAKf1aeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine sample datasets as train and test\n",
        "combined_train = concatenate_datasets([alpaca_train_mapped,  tulu_train_mapped, ultrachat_train_mapped])\n",
        "combined_test = concatenate_datasets([alpaca_test_mapped,  tulu_test_mapped, ultrachat_test_mapped])"
      ],
      "metadata": {
        "id": "FSmgsQQy1eIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2.5: Format chats as role:user/assistant and content"
      ],
      "metadata": {
        "id": "nnPSzE6l1mfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format chat as role:user/assistant and content\n",
        "def format_chat(row):\n",
        "  messages = []\n",
        "  for message in row[\"chat\"]:\n",
        "    content = \"\"\n",
        "    if message[\"system\"].strip():\n",
        "      content += message[\"system\"].strip() + \"\\n\"\n",
        "    content += message[\"instruction\"]\n",
        "    if message[\"input\"].strip():\n",
        "      content += \"\\n\" + message[\"input\"].strip()\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": content})\n",
        "    messages.append({\"role\": \"assistant\", \"content\": message[\"response\"]})\n",
        "\n",
        "  return {\"messages\": messages}\n",
        "\n",
        "\n",
        "# Format chat to a generic format and map to dataset format.\n",
        "combined_train_formatted= combined_train.map(format_chat, remove_columns=combined_train.column_names)\n",
        "combined_test_formatted= combined_test.map(format_chat, remove_columns=combined_test.column_names)"
      ],
      "metadata": {
        "id": "65TdJw2y1loM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Evaluate base model: google/gemma-3-1b-it"
      ],
      "metadata": {
        "id": "MgICNGsU18p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from tqdm import tqdm\n",
        "\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ],
      "metadata": {
        "id": "52rYC9mp2RaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3.1: Format dataset to gemma format"
      ],
      "metadata": {
        "id": "14OYT3o48BYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format message to gemma format\n",
        "def build_gemma_chat_prompt(messages):\n",
        "  prompt = \"\"\n",
        "\n",
        "  for m in messages[:-1]:\n",
        "    if m[\"role\"] == \"user\":\n",
        "      prompt += f\"<start_of_turn>user\\n{m['content']}\\n<end_of_turn>\\n\"\n",
        "    else:\n",
        "      prompt += f\"<start_of_turn>model\\n{m['content']}\\n<end_of_turn>\\n\"\n",
        "\n",
        "  last_user = None\n",
        "  for m in reversed(messages):\n",
        "    if m[\"role\"] == \"user\":\n",
        "      last_user = m[\"content\"]\n",
        "      break\n",
        "\n",
        "  prompt += (\n",
        "    f\"<start_of_turn>user\\n{last_user}\\n<end_of_turn>\\n\"\n",
        "    f\"<start_of_turn>model\\n\"\n",
        "  )\n",
        "\n",
        "  return prompt\n",
        "\n",
        "\n",
        "# Retrieve reference\n",
        "def get_reference(messages):\n",
        "  for m in reversed(messages):\n",
        "    if m[\"role\"] == \"assistant\":\n",
        "      return m[\"content\"]\n",
        "  raise ValueError(\"No assistant reference found\")\n",
        "\n",
        "\n",
        "# Generate prompts\n",
        "all_prompts = [build_gemma_chat_prompt(item[\"messages\"]) for item in combined_test_formatted]\n",
        "# Generate references\n",
        "all_refs = [get_reference(item[\"messages\"]) for item in combined_test_formatted]"
      ],
      "metadata": {
        "id": "WEoweRlw5tmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3.2: Load model and tokenizer"
      ],
      "metadata": {
        "id": "ZfDlVLwF2HiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "model_id = \"google/gemma-3-1b-it\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "AjV1-aRM15sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3.3: Generate predictions"
      ],
      "metadata": {
        "id": "VgSt8Ouo30eV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "def fast_generate(batch, max_new_tokens=64):\n",
        "  inputs = tokenizer(\n",
        "      batch,\n",
        "      return_tensors=\"pt\",\n",
        "      padding=True,\n",
        "      truncation=True,\n",
        "      max_length=2048,\n",
        "  ).to(\"cuda\")\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        use_cache=False,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "  texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "  preds = []\n",
        "  for prompt, full in zip(batch, texts):\n",
        "    preds.append(full[len(prompt):].strip())\n",
        "\n",
        "  return preds\n",
        "\n",
        "\n",
        "def evaluate_model(prompts, max_new_tokens=64):\n",
        "  all_preds = []\n",
        "  batch_size = 8\n",
        "\n",
        "  for i in tqdm(range(0, len(prompts), batch_size), desc=\"Evaluating\"):\n",
        "    batch = prompts[i:i+batch_size]\n",
        "\n",
        "    try:\n",
        "      preds = fast_generate(batch, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    except RuntimeError as e:\n",
        "      if \"out of memory\" in str(e).lower():\n",
        "        print(\"OOM — retrying batch with size=1\")\n",
        "        torch.cuda.empty_cache()\n",
        "        preds = []\n",
        "        for p in batch:\n",
        "          preds.extend(fast_generate([p], max_new_tokens=max_new_tokens))\n",
        "      else:\n",
        "        raise e\n",
        "\n",
        "    all_preds.extend(preds)\n",
        "\n",
        "  return all_preds\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "all_preds = evaluate_model(all_prompts)"
      ],
      "metadata": {
        "id": "wJbcNed64Ecm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Evaluate"
      ],
      "metadata": {
        "id": "odh3vlfy-c-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute BLEU-4\n",
        "bleu = evaluate.load(\"sacrebleu\")\n",
        "bleu4 = bleu.compute(predictions=all_preds, references=all_refs)\n",
        "print(\"BLEU-4:\", bleu4[\"score\"])\n",
        "\n",
        "# Compute ROUGE-L\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "rouge_result = rouge.compute(predictions=all_preds, references=all_refs)\n",
        "print(\"ROUGE-L:\", rouge_result[\"rougeL\"])"
      ],
      "metadata": {
        "id": "ocQaySR-9Vnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Finetune - QLoRA"
      ],
      "metadata": {
        "id": "fbscPQBU-3dK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "from peft import LoraConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForImageTextToText, BitsAndBytesConfig\n",
        "from trl import SFTConfig, SFTTrainer"
      ],
      "metadata": {
        "id": "xkLwfnQl_U-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4.1: Load model and tokenizer"
      ],
      "metadata": {
        "id": "8BQBa-q1_MPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face model id\n",
        "model_id = \"google/gemma-3-1b-it\"\n",
        "\n",
        "# Select model class based on id\n",
        "\n",
        "model_class = AutoModelForCausalLM\n",
        "\n",
        "# Check if GPU benefits from bfloat16\n",
        "if torch.cuda.get_device_capability()[0] >= 8:\n",
        "  torch_dtype = torch.bfloat16\n",
        "else:\n",
        "  torch_dtype = torch.float16\n",
        "\n",
        "# Define model init arguments\n",
        "model_kwargs = dict(\n",
        "  attn_implementation=\"eager\", # Use \"flash_attention_2\" when running on Ampere or newer GPU\n",
        "  torch_dtype=torch_dtype, # What torch dtype to use, defaults to auto\n",
        "  device_map=\"auto\", # Let torch decide how to load the model\n",
        ")\n",
        "\n",
        "# BitsAndBytesConfig: Enables 4-bit quantization to reduce model size/memory usage\n",
        "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
        "  load_in_4bit=True,\n",
        "  bnb_4bit_use_double_quant=True,\n",
        "  bnb_4bit_quant_type='nf4',\n",
        "  bnb_4bit_compute_dtype=model_kwargs['torch_dtype'],\n",
        "  bnb_4bit_quant_storage=model_kwargs['torch_dtype'],\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = model_class.from_pretrained(model_id, **model_kwargs)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\") # Load the Instruction Tokenizer to use the official Gemma template"
      ],
      "metadata": {
        "id": "Goi58SfN-pgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4.2: Prepare for finetuning"
      ],
      "metadata": {
        "id": "IElRwfQa_gl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set LoraConfig\n",
        "peft_config = LoraConfig(\n",
        "  lora_alpha=16,\n",
        "  lora_dropout=0.05,\n",
        "  r=16,\n",
        "  bias=\"none\",\n",
        "  target_modules=\"all-linear\",\n",
        "  task_type=\"CAUSAL_LM\",\n",
        "  modules_to_save=[\"lm_head\", \"embed_tokens\"] # make sure to save the lm_head and embed_tokens as you train the special tokens\n",
        ")\n",
        "\n",
        "# Set SFTConfig\n",
        "args = SFTConfig(\n",
        "  output_dir=\"./qlora-gemma\",         # directory to save and repository id\n",
        "  max_length=2048,                         # max sequence length for model and packing of the dataset\n",
        "  packing=True,                           # Groups multiple samples in the dataset into a single sequence\n",
        "  num_train_epochs=2,                     # number of training epochs\n",
        "  per_device_train_batch_size=1,          # batch size per device during training\n",
        "  gradient_accumulation_steps=4,          # number of steps before performing a backward/update pass\n",
        "  gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
        "  optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
        "  logging_steps=10,                       # log every 10 steps\n",
        "  save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
        "  learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
        "  fp16=True if torch_dtype == torch.float16 else False,   # use float16 precision\n",
        "  bf16=True if torch_dtype == torch.bfloat16 else False,   # use bfloat16 precision\n",
        "  max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
        "  warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
        "  lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
        "  push_to_hub=False,                       # push model to hub\n",
        "  report_to=\"tensorboard\",                # report metrics to tensorboard\n",
        "  dataset_kwargs={\n",
        "      \"add_special_tokens\": False, # We template with special tokens\n",
        "      \"append_concat_token\": True, # Add EOS token as separator token between examples\n",
        "  }\n",
        ")\n",
        "\n",
        "# Create Trainer object\n",
        "trainer = SFTTrainer(\n",
        "  model=model,\n",
        "  args=args,\n",
        "  train_dataset=combined_train_formatted,\n",
        "  peft_config=peft_config,\n",
        "  processing_class=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "FiA85lIF_czX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4.3: Finetune"
      ],
      "metadata": {
        "id": "BHiDOwt9AUeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.reset_peak_memory_stats()\n",
        "start_time = time.time()\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Compute peak memory\n",
        "peak_memory = torch.cuda.max_memory_allocated() / 1024**3  # in GB\n",
        "# Compute training time\n",
        "training_time = end_time - start_time  # in seconds\n",
        "\n",
        "print(f\"Peak GPU memory: {peak_memory:.2f} GB\")\n",
        "print(f\"Total training time: {training_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "WdxqO-pTAQ26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4:4: Save model"
      ],
      "metadata": {
        "id": "tfYSxTuMA6Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "OsXPWSbOA_jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Evaluate QLoRA"
      ],
      "metadata": {
        "id": "fvZQt9GWBYAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from tqdm import tqdm\n",
        "\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ],
      "metadata": {
        "id": "AQm7crsaBcTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5.1: Format dataset to gemma format"
      ],
      "metadata": {
        "id": "OC6QM2L3B_oE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format message to gemma format\n",
        "def build_gemma_chat_prompt(messages):\n",
        "  prompt = \"\"\n",
        "\n",
        "  for m in messages[:-1]:\n",
        "    if m[\"role\"] == \"user\":\n",
        "      prompt += f\"<start_of_turn>user\\n{m['content']}\\n<end_of_turn>\\n\"\n",
        "    else:\n",
        "      prompt += f\"<start_of_turn>model\\n{m['content']}\\n<end_of_turn>\\n\"\n",
        "\n",
        "  last_user = None\n",
        "  for m in reversed(messages):\n",
        "    if m[\"role\"] == \"user\":\n",
        "      last_user = m[\"content\"]\n",
        "      break\n",
        "\n",
        "  prompt += (\n",
        "    f\"<start_of_turn>user\\n{last_user}\\n<end_of_turn>\\n\"\n",
        "    f\"<start_of_turn>model\\n\"\n",
        "  )\n",
        "\n",
        "  return prompt\n",
        "\n",
        "# Retrieve reference\n",
        "def get_reference(messages):\n",
        "  for m in reversed(messages):\n",
        "    if m[\"role\"] == \"assistant\":\n",
        "      return m[\"content\"]\n",
        "  raise ValueError(\"No assistant reference found\")\n",
        "\n",
        "\n",
        "# Generate prompts\n",
        "all_prompts = [build_gemma_chat_prompt(item[\"messages\"]) for item in combined_test_formatted]\n",
        "# Generate references\n",
        "all_refs = [get_reference(item[\"messages\"]) for item in combined_test_formatted]"
      ],
      "metadata": {
        "id": "zqtTjxATByv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5.2: Load model"
      ],
      "metadata": {
        "id": "XGvmsqbiCSzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load fine-tuned model\n",
        "model = PeftModel.from_pretrained(model, \"./qlora-gemma\")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "D4awu-4npCZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5.3: Generate predictions"
      ],
      "metadata": {
        "id": "jt15oxTZCxKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "def fast_generate(batch, max_new_tokens=64):\n",
        "  inputs = tokenizer(\n",
        "      batch,\n",
        "      return_tensors=\"pt\",\n",
        "      padding=True,\n",
        "      truncation=True,\n",
        "      max_length=2048,\n",
        "  ).to(\"cuda\")\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        use_cache=False,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "  texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "  preds = []\n",
        "  for prompt, full in zip(batch, texts):\n",
        "    preds.append(full[len(prompt):].strip())\n",
        "\n",
        "  return preds\n",
        "\n",
        "\n",
        "def evaluate_model(prompts, max_new_tokens=64):\n",
        "  all_preds = []\n",
        "  batch_size = 8\n",
        "\n",
        "  for i in tqdm(range(0, len(prompts), batch_size), desc=\"Evaluating\"):\n",
        "    batch = prompts[i:i+batch_size]\n",
        "\n",
        "    try:\n",
        "      preds = fast_generate(batch, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    except RuntimeError as e:\n",
        "      if \"out of memory\" in str(e).lower():\n",
        "        print(\"OOM — retrying batch with size=1\")\n",
        "        torch.cuda.empty_cache()\n",
        "        preds = []\n",
        "        for p in batch:\n",
        "          preds.extend(fast_generate([p], max_new_tokens=max_new_tokens))\n",
        "      else:\n",
        "        raise e\n",
        "\n",
        "    all_preds.extend(preds)\n",
        "\n",
        "  return all_preds\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "all_preds = evaluate_model(all_prompts)"
      ],
      "metadata": {
        "id": "EfloDDp8Cqud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5.4: Evaluate"
      ],
      "metadata": {
        "id": "wi2KLPGHC8s2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute BLEU-4\n",
        "bleu = evaluate.load(\"sacrebleu\")\n",
        "bleu4 = bleu.compute(predictions=all_preds, references=all_refs)\n",
        "print(\"BLEU-4:\", bleu4[\"score\"])\n",
        "\n",
        "# Compute ROUGE-L\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "rouge_result = rouge.compute(predictions=all_preds, references=all_refs)\n",
        "print(\"ROUGE-L:\", rouge_result[\"rougeL\"])"
      ],
      "metadata": {
        "id": "8FoGAU-7C4O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Finetune LoRA"
      ],
      "metadata": {
        "id": "yap5Mi6UDRST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "from peft import LoraConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForImageTextToText, BitsAndBytesConfig\n",
        "from trl import SFTConfig, SFTTrainer"
      ],
      "metadata": {
        "id": "bCUn7HAjDIkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6.1: Load model and tokenizer"
      ],
      "metadata": {
        "id": "RbYZ_MQTDiwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face model id\n",
        "model_id = \"google/gemma-3-1b-it\" # or `google/gemma-3-4b-pt`, `google/gemma-3-12b-pt`, `google/gemma-3-27b-pt`\n",
        "\n",
        "# Select model class based on id\n",
        "\n",
        "model_class = AutoModelForCausalLM\n",
        "\n",
        "# Check if GPU benefits from bfloat16\n",
        "if torch.cuda.get_device_capability()[0] >= 8:\n",
        "  torch_dtype = torch.bfloat16\n",
        "else:\n",
        "  torch_dtype = torch.float16\n",
        "\n",
        "# Define model init arguments\n",
        "model_kwargs = dict(\n",
        "  attn_implementation=\"eager\", # Use \"flash_attention_2\" when running on Ampere or newer GPU\n",
        "  torch_dtype=torch_dtype, # What torch dtype to use, defaults to auto\n",
        "  device_map=\"auto\", # Let torch decide how to load the model\n",
        ")\n",
        "\n",
        "# BitsAndBytesConfig: Enables 4-bit quantization to reduce model size/memory usage\n",
        "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
        "  load_in_4bit=True,\n",
        "  bnb_4bit_use_double_quant=True,\n",
        "  bnb_4bit_quant_type='nf4',\n",
        "  bnb_4bit_compute_dtype=model_kwargs['torch_dtype'],\n",
        "  bnb_4bit_quant_storage=model_kwargs['torch_dtype'],\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = model_class.from_pretrained(model_id, **model_kwargs)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\") # Load the Instruction Tokenizer to use the official Gemma template"
      ],
      "metadata": {
        "id": "hZ4z2UQ6DcV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6.2: Prepare for finetuning"
      ],
      "metadata": {
        "id": "EWr2TE2fDxe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set LoraConfig\n",
        "peft_config = LoraConfig(\n",
        "  r=16,\n",
        "  lora_alpha=16,\n",
        "  lora_dropout=0.05,\n",
        "  target_modules=[\"q_proj\", \"v_proj\"],\n",
        "  bias=\"none\",\n",
        "  task_type=\"CAUSAL_LM\",\n",
        "  modules_to_save=[\"lm_head\", \"embed_tokens\"]\n",
        ")\n",
        "\n",
        "# Set SFTConfig\n",
        "args = SFTConfig(\n",
        "  output_dir=\"./lora-gemma\",         # directory to save and repository id\n",
        "  max_length=2048,                         # max sequence length for model and packing of the dataset\n",
        "  packing=True,                           # Groups multiple samples in the dataset into a single sequence\n",
        "  num_train_epochs=2,                     # number of training epochs\n",
        "  per_device_train_batch_size=1,          # batch size per device during training\n",
        "  gradient_accumulation_steps=4,          # number of steps before performing a backward/update pass\n",
        "  gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
        "  optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
        "  logging_steps=10,                       # log every 10 steps\n",
        "  save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
        "  learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
        "  fp16=True if torch_dtype == torch.float16 else False,   # use float16 precision\n",
        "  bf16=True if torch_dtype == torch.bfloat16 else False,   # use bfloat16 precision\n",
        "  max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
        "  warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
        "  lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
        "  push_to_hub=False,                       # push model to hub\n",
        "  report_to=\"tensorboard\",                # report metrics to tensorboard\n",
        "  dataset_kwargs={\n",
        "      \"add_special_tokens\": False, # We template with special tokens\n",
        "      \"append_concat_token\": True, # Add EOS token as separator token between examples\n",
        "  }\n",
        ")\n",
        "\n",
        "# Create Trainer object\n",
        "trainer = SFTTrainer(\n",
        "  model=model,\n",
        "  args=args,\n",
        "  train_dataset=combined_train_formatted,\n",
        "  peft_config=peft_config,\n",
        "  processing_class=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "ckpu2iCNEhuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6.3: Finetune"
      ],
      "metadata": {
        "id": "GLTjoREhEQGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.reset_peak_memory_stats()\n",
        "start_time = time.time()\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Compute peak memory\n",
        "peak_memory = torch.cuda.max_memory_allocated() / 1024**3  # in GB\n",
        "# Compute training time\n",
        "training_time = end_time - start_time  # in seconds\n",
        "\n",
        "print(f\"Peak GPU memory: {peak_memory:.2f} GB\")\n",
        "print(f\"Total training time: {training_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "TdDTtiG9ECwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6.4: Save model"
      ],
      "metadata": {
        "id": "OT2JRW3BEU0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "NcpJFoUWETvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Evaluate LoRA"
      ],
      "metadata": {
        "id": "4006tMhiEfoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from tqdm import tqdm\n",
        "\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ],
      "metadata": {
        "id": "mS79DZnVEeqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7.1: Format dataset to gemma format"
      ],
      "metadata": {
        "id": "-atYn-BREwUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format message to gemma format\n",
        "def build_gemma_chat_prompt(messages):\n",
        "  prompt = \"\"\n",
        "\n",
        "  for m in messages[:-1]:\n",
        "    if m[\"role\"] == \"user\":\n",
        "      prompt += f\"<start_of_turn>user\\n{m['content']}\\n<end_of_turn>\\n\"\n",
        "    else:\n",
        "      prompt += f\"<start_of_turn>model\\n{m['content']}\\n<end_of_turn>\\n\"\n",
        "\n",
        "  last_user = None\n",
        "  for m in reversed(messages):\n",
        "    if m[\"role\"] == \"user\":\n",
        "      last_user = m[\"content\"]\n",
        "      break\n",
        "\n",
        "  prompt += (\n",
        "    f\"<start_of_turn>user\\n{last_user}\\n<end_of_turn>\\n\"\n",
        "    f\"<start_of_turn>model\\n\"\n",
        "  )\n",
        "\n",
        "  return prompt\n",
        "\n",
        "# Retrieve reference\n",
        "def get_reference(messages):\n",
        "  for m in reversed(messages):\n",
        "    if m[\"role\"] == \"assistant\":\n",
        "      return m[\"content\"]\n",
        "  raise ValueError(\"No assistant reference found\")\n",
        "\n",
        "# Generate prompts\n",
        "all_prompts = [build_gemma_chat_prompt(item[\"messages\"]) for item in combined_test_formatted]\n",
        "# Generate references\n",
        "all_refs = [get_reference(item[\"messages\"]) for item in combined_test_formatted]"
      ],
      "metadata": {
        "id": "RpV-FPHbErO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7.2: Load model"
      ],
      "metadata": {
        "id": "kRFzJcnuE7AO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load fine-tuned model\n",
        "model = PeftModel.from_pretrained(model, \"./lora-gemma\")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "Vi6hODhXl1OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7.3: Generate predictions"
      ],
      "metadata": {
        "id": "1gc_vuotFJdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "def fast_generate(batch, max_new_tokens=64):\n",
        "  inputs = tokenizer(\n",
        "      batch,\n",
        "      return_tensors=\"pt\",\n",
        "      padding=True,\n",
        "      truncation=True,\n",
        "      max_length=2048,\n",
        "  ).to(\"cuda\")\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        use_cache=False,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "  texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "  preds = []\n",
        "  for prompt, full in zip(batch, texts):\n",
        "    preds.append(full[len(prompt):].strip())\n",
        "\n",
        "  return preds\n",
        "\n",
        "\n",
        "def evaluate_model(prompts, max_new_tokens=64):\n",
        "  all_preds = []\n",
        "  batch_size = 8\n",
        "\n",
        "  for i in tqdm(range(0, len(prompts), batch_size), desc=\"Evaluating\"):\n",
        "    batch = prompts[i:i+batch_size]\n",
        "\n",
        "    try:\n",
        "      preds = fast_generate(batch, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    except RuntimeError as e:\n",
        "      if \"out of memory\" in str(e).lower():\n",
        "        print(\"OOM — retrying batch with size=1\")\n",
        "        torch.cuda.empty_cache()\n",
        "        preds = []\n",
        "        for p in batch:\n",
        "          preds.extend(fast_generate([p], max_new_tokens=max_new_tokens))\n",
        "      else:\n",
        "        raise e\n",
        "\n",
        "    all_preds.extend(preds)\n",
        "\n",
        "  return all_preds\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "all_preds = evaluate_model(all_prompts)"
      ],
      "metadata": {
        "id": "I8hfpTa1FJLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7.4 Evaluate"
      ],
      "metadata": {
        "id": "nmyAlEa1FaVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute BLEU-4\n",
        "bleu = evaluate.load(\"sacrebleu\")\n",
        "bleu4 = bleu.compute(predictions=all_preds, references=all_refs)\n",
        "print(\"BLEU-4:\", bleu4[\"score\"])\n",
        "\n",
        "# Compute ROUGE-L\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "rouge_result = rouge.compute(predictions=all_preds, references=all_refs)\n",
        "print(\"ROUGE-L:\", rouge_result[\"rougeL\"])"
      ],
      "metadata": {
        "id": "IlJWxzRPFLPu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}