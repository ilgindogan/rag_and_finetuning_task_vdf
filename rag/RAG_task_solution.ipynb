{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Prepare Environment"
      ],
      "metadata": {
        "id": "Cg2c3Xwtuqzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q sentence-transformers qdrant-client transformers accelerate bitsandbytes evaluate rouge_score nltk sacrebleu"
      ],
      "metadata": {
        "id": "c-9l_NpmhNT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import re\n",
        "import requests\n",
        "import shutil\n",
        "import torch\n",
        "import time\n",
        "import uuid\n",
        "\n",
        "from datasets import load_dataset\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, PointStruct, ScoredPoint, VectorParams\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "iglFPQvohgVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Enter your Hugging Face access token to be able to use Gemma"
      ],
      "metadata": {
        "id": "R5slew_LuyoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ],
      "metadata": {
        "id": "XZ66OJqLiUfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Data Selection & Preparation"
      ],
      "metadata": {
        "id": "utofrNzNu3TF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selected book\n",
        "title = \"The Tale of Two Bad Mice\"\n",
        "document_id = \"08c3429e9717bc663bc5f41cd3a2c701c222ed2f\"\n",
        "story_url = \"http://www.gutenberg.org/ebooks/45264.txt.utf-8\"\n",
        "\n",
        "\n",
        "print(\"title:\", title)\n",
        "print(\"document_id: \", document_id)\n",
        "print(\"story_url: \", story_url)"
      ],
      "metadata": {
        "id": "C7f6IrYys2fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the text\n",
        "response = requests.get(story_url)\n",
        "response.raise_for_status()\n",
        "raw_text = response.text\n",
        "\n",
        "print(\"Downloaded characters:\", len(raw_text))\n",
        "\n",
        "# Cleanthe text\n",
        "def clean_gutenberg(text):\n",
        "  # Remove header\n",
        "  start_pattern = r\"\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK THE TALE OF TWO BAD MICE \\*\\*\\*\"\n",
        "  start_match = re.search(start_pattern, text, re.IGNORECASE)\n",
        "  if start_match:\n",
        "    text = text[start_match.end():]\n",
        "  else:\n",
        "    print(\"START marker not found\")\n",
        "\n",
        "  # Remove footer\n",
        "  end_pattern = r\"\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "  end_match = re.search(end_pattern, text, re.IGNORECASE)\n",
        "  if end_match:\n",
        "    text = text[:end_match.start()]\n",
        "  else:\n",
        "    print(\"END marker not found\")\n",
        "\n",
        "  # Additional cleanining from the start\n",
        "  story_start = re.search(r\"THE TALE OF TWO BAD MICE\", text, re.IGNORECASE)\n",
        "  if story_start:\n",
        "    text = text[story_start.start():]\n",
        "\n",
        "  story_start = re.search(r\"ONCE\", text, re.IGNORECASE)\n",
        "  if story_start:\n",
        "    text = text[story_start.start():]\n",
        "\n",
        "  # Additional cleanining from the end\n",
        "  story_end = re.search(r\"PRINTED BY\", text, re.IGNORECASE)\n",
        "  if story_end:\n",
        "    text = text[:story_end.start()]\n",
        "\n",
        "  # Clean Illustration\n",
        "  text = text.replace(\"[Illustration]\", \"\")\n",
        "\n",
        "  # Clean whitespace\n",
        "  text = text.strip()\n",
        "  text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)\n",
        "\n",
        "  return text\n",
        "\n",
        "# Apply cleaning\n",
        "clean_text = clean_gutenberg(raw_text)\n",
        "\n",
        "# Save cleaned text to a file\n",
        "with open(\"The_Tale_of_Two_Bad_Mice_clean.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "  f.write(clean_text)\n",
        "print(\"Cleaned text saved to The_Tale_of_Two_Bad_Mice_clean.txt\")\n"
      ],
      "metadata": {
        "id": "zgOKvbJi7Yf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load NarrativeQA dataset from Hugging Face\n",
        "narrativeqa = load_dataset(\"narrativeqa\")"
      ],
      "metadata": {
        "id": "7uuulExS0yIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter test set for your selected book using document_id\n",
        "test_filtered = narrativeqa['test'].filter(\n",
        "    lambda x: x['document']['id'] == document_id\n",
        ")\n",
        "\n",
        "# Print number of QA test pairs\n",
        "print(f\"\\nNumber of QA pairs in TEST set for '{title}': {len(test_filtered)}\")\n",
        "\n",
        "# Preview QA pairs from test set\n",
        "print(\"\\n--- Sample QA pairs from TEST set ---\")\n",
        "for i, example in enumerate(test_filtered):\n",
        "  print(f\"\\nQ{i+1}: {example['question']['text']}\")\n",
        "  print(f\"A{i+1}.1: {example['answers'][0]['text']}\")\n",
        "  if len(example['answers']) > 1:\n",
        "    print(f\"A{i+1}.2: {example['answers'][1]['text']}\")"
      ],
      "metadata": {
        "id": "YqlRjKSg7ggB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"The_Tale_of_Two_Bad_Mice_clean.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  clean_text = f.read()\n",
        "\n",
        "print(f\"Total characters: {len(clean_text)}\")\n",
        "print(f\"Total words: {len(clean_text.split())}\")"
      ],
      "metadata": {
        "id": "CvH4tdFf7oLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Indexing Strategy: Hierarchical Chunking"
      ],
      "metadata": {
        "id": "Cuvlz2DzwaYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunking\n",
        "def fast_chunk(text, parent_size=1000, child_size=200, overlap=20):\n",
        "  parents, children = [], []\n",
        "  for p_idx, p_start in enumerate(range(0, len(text), parent_size)):\n",
        "    p_text = text[p_start:p_start + parent_size].strip()\n",
        "    if not p_text:\n",
        "      continue\n",
        "    parents.append({'id': f'p{p_idx}', 'text': p_text})\n",
        "    for c_idx, c_start in enumerate(range(0, len(p_text), child_size - overlap)):\n",
        "      c_text = p_text[c_start:c_start + child_size].strip()\n",
        "      if c_text:\n",
        "        children.append({'id': f'p{p_idx}_c{c_idx}', 'parent_id': f'p{p_idx}', 'text': c_text})\n",
        "  return parents, children\n",
        "\n",
        "parent_chunks, child_chunks = fast_chunk(clean_text)\n",
        "print(f\"Parents: {len(parent_chunks)}, Children: {len(child_chunks)}\")\n",
        "\n",
        "with open(\"parent_chunks.json\", \"w\") as f:\n",
        "  json.dump({p['id']: p['text'] for p in parent_chunks}, f)"
      ],
      "metadata": {
        "id": "_dieVWux7q5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Embed one at a time to avoid RAM spike\n",
        "child_embeddings = []\n",
        "for c in child_chunks:\n",
        "  emb = model.encode(c['text'], convert_to_numpy=True)\n",
        "  child_embeddings.append(emb)\n",
        "\n",
        "print(f\"Done: {len(child_embeddings)} embeddings\")"
      ],
      "metadata": {
        "id": "uRiutQ-m79-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Retrieval"
      ],
      "metadata": {
        "id": "r4lC0Z4TyOxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DB\n",
        "# Remove old database folder to release lock\n",
        "if os.path.exists(\"./qdrant_db\"):\n",
        "  shutil.rmtree(\"./qdrant_db\")\n",
        "\n",
        "client = QdrantClient(path=\"./qdrant_db\")\n",
        "\n",
        "client.create_collection(\"bad_mice\", vectors_config=VectorParams(size=384, distance=Distance.COSINE))\n",
        "\n",
        "points = [\n",
        "  PointStruct(id=i, vector=child_embeddings[i].tolist(),\n",
        "              payload={'id': child_chunks[i]['id'], 'parent_id': child_chunks[i]['parent_id'], 'text': child_chunks[i]['text']})\n",
        "  for i in range(len(child_chunks))\n",
        "]\n",
        "\n",
        "client.upsert(collection_name=\"bad_mice\", points=points)\n",
        "print(f\"Indexed {len(points)} chunks\")"
      ],
      "metadata": {
        "id": "qDe9GP3v8A6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_id = \"08c3429e9717bc663bc5f41cd3a2c701c222ed2f\"\n",
        "\n",
        "# Extract QA pairs\n",
        "qa_pairs = []\n",
        "for ex in test_filtered:\n",
        "  qa_pairs.append({\n",
        "    'question': ex['question']['text'],\n",
        "    'answers': [a['text'] for a in ex['answers']]\n",
        "  })\n",
        "\n",
        "print(f\"Sample Q: {qa_pairs[0]['question']}\")\n",
        "print(f\"Sample A: {qa_pairs[0]['answers']}\")"
      ],
      "metadata": {
        "id": "inJBWWcU8N8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"parent_chunks.json\", \"r\") as f:\n",
        "  parent_lookup = json.load(f)\n",
        "\n",
        "# Retrieval: Retrieve top-k child chunks and optionally expand to parent context\n",
        "def retrieve_with_context(question, top_k=3, use_parent=True):\n",
        "  query_vec = model.encode(question).tolist()\n",
        "\n",
        "  results = client.query_points(\n",
        "    collection_name=\"bad_mice\",\n",
        "    query=query_vec,\n",
        "    limit=top_k\n",
        "  )\n",
        "\n",
        "  child_texts = []\n",
        "  parent_ids = set()\n",
        "\n",
        "  for r in results.points:\n",
        "    child_texts.append(r.payload['text'])\n",
        "    parent_ids.add(r.payload['parent_id'])\n",
        "\n",
        "  if use_parent:\n",
        "    # Use parent chunks for expanded context\n",
        "    context = \"\\n\\n\".join([parent_lookup[pid] for pid in sorted(parent_ids)])\n",
        "  else:\n",
        "    # Use only child chunks\n",
        "    context = \"\\n\\n\".join(child_texts)\n",
        "\n",
        "  return context, child_texts, list(parent_ids)\n",
        "\n",
        "# Test retrieval\n",
        "ctx, children, parents = retrieve_with_context(\"What did the mice do?\")\n",
        "print(f\"Retrieved {len(children)} children from {len(parents)} parents\")\n",
        "print(f\"Context length: {len(ctx)} chars\")"
      ],
      "metadata": {
        "id": "G-PsECTK8QFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set LLM model\n",
        "model_name = \"google/gemma-3-1b-it\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "  model_name,\n",
        "  torch_dtype=torch.float16,\n",
        "  device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(\"Gemma loaded\")"
      ],
      "metadata": {
        "id": "-S7eOyM-8TQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creare RAG prompt\n",
        "def create_prompt(context, question):\n",
        "  prompt = f\"\"\"<start_of_turn>user\n",
        "Read the context and answer the question in 1-3 words only.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer in 1-3 words only:\n",
        "<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "  return prompt\n",
        "\n",
        "\n",
        "# Create baseline prompt\n",
        "def create_baseline_prompt(question):\n",
        "  prompt = f\"\"\"<start_of_turn>user\n",
        "Answer the question in 1-3 words only: {question}\n",
        "<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "  return prompt\n",
        "\n",
        "\n",
        "# Generate answer\n",
        "def generate_answer(prompt, max_new_tokens=100):\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(llm.device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = llm.generate(\n",
        "      **inputs,\n",
        "      max_new_tokens=max_new_tokens,\n",
        "      do_sample=False,\n",
        "      pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "  response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  answer = response.split(\"model\")[-1].strip()\n",
        "  return answer\n",
        "\n",
        "# Test generation\n",
        "test_ctx, _, _ = retrieve_with_context(qa_pairs[0]['question'])\n",
        "test_prompt = create_prompt(test_ctx, qa_pairs[0]['question'])\n",
        "test_answer = generate_answer(test_prompt)\n",
        "print(f\"Question: {qa_pairs[0]['question']}\")\n",
        "print(f\"Generated: {test_answer}\")\n",
        "print(f\"Reference: {qa_pairs[0]['answers']}\")"
      ],
      "metadata": {
        "id": "krdxVfiJ8Vld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load metrics\n",
        "bleu = evaluate.load(\"sacrebleu\")\n",
        "rouge = evaluate.load(\"rouge\")"
      ],
      "metadata": {
        "id": "RcVRc9Bv9xys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get current RAM usage in MB\n",
        "def get_memory_usage():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  return process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "\n",
        "# Get disk usage of a folder in MB\n",
        "def get_disk_usage(path=\"./qdrant_db\"):\n",
        "  total_size = 0\n",
        "  if os.path.exists(path):\n",
        "    for dirpath, dirnames, filenames in os.walk(path):\n",
        "      for f in filenames:\n",
        "        fp = os.path.join(dirpath, f)\n",
        "        total_size += os.path.getsize(fp)\n",
        "  return total_size / 1024 / 1024\n",
        "\n",
        "\n",
        "# Print current resource usage\n",
        "def monitor_resources():\n",
        "  ram = get_memory_usage()\n",
        "  disk = get_disk_usage()\n",
        "  print(f\"RAM: {ram:.1f} MB | Qdrant DB: {disk:.2f} MB\")\n",
        "  return ram, disk\n",
        "\n",
        "print(\"Initial state:\")\n",
        "monitor_resources()"
      ],
      "metadata": {
        "id": "_XfPKe20g8Pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_preds = []\n",
        "rag_preds = []\n",
        "all_refs = []\n",
        "\n",
        "# Resource tracking\n",
        "peak_ram = 0\n",
        "inference_times = {'baseline': [], 'rag': []}\n",
        "\n",
        "print(\"Running evaluation...\")\n",
        "\n",
        "for qa in tqdm(qa_pairs):\n",
        "  question = qa['question']\n",
        "  references = qa['answers']\n",
        "\n",
        "  # Store references\n",
        "  all_refs.append(references)\n",
        "\n",
        "  # Baseline (no RAG)\n",
        "  start = time.time()\n",
        "  baseline_prompt = create_baseline_prompt(question)\n",
        "  baseline_answer = generate_answer(baseline_prompt)\n",
        "  inference_times['baseline'].append(time.time() - start)\n",
        "  baseline_preds.append(baseline_answer)\n",
        "\n",
        "  # RAG\n",
        "  start = time.time()\n",
        "  context, _, _ = retrieve_with_context(question, top_k=3, use_parent=True)\n",
        "  rag_prompt = create_prompt(context, question)\n",
        "  rag_answer = generate_answer(rag_prompt)\n",
        "  inference_times['rag'].append(time.time() - start)\n",
        "  rag_preds.append(rag_answer)\n",
        "\n",
        "  # Track peak RAM\n",
        "  current_ram = get_memory_usage()\n",
        "  peak_ram = max(peak_ram, current_ram)\n",
        "\n",
        "print(\"Evaluation complete!\")"
      ],
      "metadata": {
        "id": "d2jyBk-090Tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Number of test QA pairs: {len(qa_pairs)}\")\n",
        "\n",
        "# Baseline metrics\n",
        "print(f\"\\n--- BASELINE (No RAG) ---\")\n",
        "baseline_bleu = bleu.compute(predictions=baseline_preds, references=all_refs, lowercase=True)\n",
        "baseline_rouge = rouge.compute(predictions=baseline_preds, references=all_refs)\n",
        "print(f\"BLEU-4:  {baseline_bleu['score']:.4f}\")\n",
        "print(f\"ROUGE-L: {baseline_rouge['rougeL']:.4f}\")\n",
        "\n",
        "print(baseline_preds)\n",
        "\n",
        "# RAG metrics\n",
        "print(f\"\\n--- RAG SYSTEM ---\")\n",
        "rag_bleu = bleu.compute(predictions=rag_preds, references=all_refs, lowercase=True)\n",
        "rag_rouge = rouge.compute(predictions=rag_preds, references=all_refs)\n",
        "print(f\"BLEU-4:  {rag_bleu['score']:.4f}\")\n",
        "print(f\"ROUGE-L: {rag_rouge['rougeL']:.4f}\")\n",
        "\n",
        "print(rag_preds)\n",
        "\n",
        "# Improvement\n",
        "print(f\"\\n--- IMPROVEMENT ---\")\n",
        "print(f\"BLEU-4:  {rag_bleu['score'] - baseline_bleu['score']:+.4f}\")\n",
        "print(f\"ROUGE-L: {rag_rouge['rougeL'] - baseline_rouge['rougeL']:+.4f}\")"
      ],
      "metadata": {
        "id": "lwA5vMUL95UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"SAMPLE COMPARISONS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i in range(len(qa_pairs)):\n",
        "  print(f\"\\n--- Example {i+1} ---\")\n",
        "  print(f\"Question: {qa_pairs[i]['question']}\")\n",
        "  print(f\"Reference: {all_refs[i]}\")\n",
        "  print(f\"Baseline: {baseline_preds[i]}\")\n",
        "  print(f\"RAG: {rag_preds[i]}\")"
      ],
      "metadata": {
        "id": "wZ6oFAFx-BR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"RESOURCE MONITORING REPORT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Memory\n",
        "print(f\"\\n--- MEMORY (RAM) ---\")\n",
        "print(f\"Peak RAM usage: {peak_ram:.1f} MB\")\n",
        "print(f\"Final RAM usage: {get_memory_usage():.1f} MB\")\n",
        "print(f\"Colab free tier limit: ~12,000 MB\")\n",
        "print(f\"Usage: {peak_ram/12000*100:.1f}% of limit\")\n",
        "\n",
        "# Disk\n",
        "print(f\"\\n--- DISK (Vector DB) ---\")\n",
        "db_size = get_disk_usage(\"./qdrant_db\")\n",
        "print(f\"Qdrant DB size: {db_size:.2f} MB\")\n",
        "print(f\"Parent chunks JSON: {os.path.getsize('parent_chunks.json')/1024:.2f} KB\")\n",
        "\n",
        "# Inference time\n",
        "print(f\"\\n--- INFERENCE TIME ---\")\n",
        "print(f\"Baseline avg: {np.mean(inference_times['baseline']):.2f}s per question\")\n",
        "print(f\"RAG avg: {np.mean(inference_times['rag']):.2f}s per question\")\n",
        "print(f\"RAG overhead: {np.mean(inference_times['rag']) - np.mean(inference_times['baseline']):.2f}s\")\n",
        "print(f\"Total baseline time: {sum(inference_times['baseline']):.1f}s\")\n",
        "print(f\"Total RAG time: {sum(inference_times['rag']):.1f}s\")"
      ],
      "metadata": {
        "id": "t1qSUq5__boJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}